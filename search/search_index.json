{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"The Data Introspection Project","text":"<p>The Data Introspection Project is a collection of tools and ideas centered around the goal of reclaiming personal value from our online information.</p> <p> A graph generated in R using <code>ggplot</code> to show the mean sentiment score month by month across Facebook and LinkedIn</p> <p>Data introspection is the practice of using data as a playground for personal development. Through a data introspection practice, an individual collects, analyzes, and reflects on their digital footprint in order to surface insights about their habits, preferences, patterns, growth, and well-being.</p>"},{"location":"#big-tech-has-warped-our-relationship-to-our-information","title":"Big Tech has warped our relationship to our information","text":"<p>Our digital lives are increasingly noisy, shaped by the gigabytes of information that we consume on a daily basis. The bits and bytes that make their way into our psychology via messages, articles, videos, shorts, and podcasts are algorithmically curated to latch into relevancy. Platforms that serve information to us use our attention to personalize recommendations and advertisements, leaving us in a swirling vortex of information.</p> <p>Whether we realize it or not, we all house data and information in some way. Our browsing habits, social media posts, and online preferences create a trove personal data, rich with insights about who we are as individuals. Our health records, held by our doctors, or our LinkedIn profiles, our old Facebook messages \u2013 they all store information about us that can reveal more than we might expect.</p> <p>This data is the lifeblood of artificial intelligence. Machine learning algorithms that power ChatGPT, Copilot, and Gemini find patterns in internet-scale pools of content that are then used as the ultimate probabilistic generator, capable of cutting through the noise that exists within our online footprints.</p> <p>Today, most social media algorithms are designed to maximize attention. User interfaces prioritize easy, repeatable interactions that keep users engaged. Some apps utilize controversial dark patterns to make users feel urgency or anxiety, nudging them into actions they might not have taken if the interface were more transparent or respectful of their intent. But these defaults can be challenged, especially in a world where we make introspection a design goal of the tools we are building in this age of emergent interfaces.</p>"},{"location":"#personal-ai-and-data-introspection","title":"Personal AI and Data Introspection","text":"<p>My initial explorations into using AI for sentiment analysis began with Archivist, a tool that allowed me to analyze my past messages. As I began to explore my past with the help of locally running models through Ollama, I found myself curious about the patterns that my messages might reveal about me. As I began constructing a personal data archive and database for myself, I built a pipeline to use <code>llama3.1</code> to assign a single sentiment value to every message in my archives, and then score that word on a scale of 0 (most negative sentiment) and 1 (most positive sentiment). From there, I've been analyzing and visualizing the information, growing the database with additional platforms, and - with these docs - sharing it with the world.</p>"},{"location":"local-agent/","title":"Using Ollama for Sentiment Analysis","text":""},{"location":"local-agent/#sentiment-analysis","title":"Sentiment Analysis","text":"<p>Sentiment analysis is the practice of assessing the likely attitude or opinion expressed through natural language. In machine learning, sentiment analysis is used to estimate emotion present in text/image/video, and can be used to classify postive, neutral, or negative feelings.</p> <p>Note: We would be remiss to not call out the fact that machine learning sentiment analysis is not without flaws. Using AI to label sentiment on posts introduces a level of bias and randomness. We chose to proceed given the size of our personal dataset, but take the output with caution.</p>"},{"location":"local-agent/#using-ollama","title":"Using Ollama","text":"<p>The Data Introspection Project contains several scripts that use Ollama to create entries in your personal database that:</p> <ol> <li>Assigns a single word <code>sentiment</code> to each row in your database</li> <li>Assigns a <code>sentiment_score</code> to each row in the database based on the single word <code>sentiment</code> stored for each message</li> </ol> <p>The <code>sentiment.py</code> script populates the sentiment string through a long series of local Ollama calls. We ran this using <code>llama3.1:latest</code>, which has 8B parameters and a context window of 128k characters. The model size is 4.9GB on disk.</p> <p>The script parses through each row of the <code>db.sqlite</code> database <code>content</code> table and sends the <code>message</code> string in a request to Ollama with the prompt: \"Give a one-word sentiment that best describes the following message. Respond with only one word.\"</p> <p>For a ~200k row database, it took roughly three hours to compute the <code>sentiment</code> for the entire database for sentiment only, and another three hours to compute the <code>sentiment_score</code> on a machine with two Nvidia 4090 GPUs and 64GB of RAM. This translated to just under 30M tokens for assigning sentiment labels.</p> <p>The <code>sentiment_score.py</code> script is essentially the exact same script, but instead of sending the message to Ollama and asking for a word describing the sentiment, it sends the sentiment and asks for a score between 0-1.</p>"},{"location":"local-agent/#sample-assessment","title":"Sample Assessment","text":"<p>The following shows a few examples of how llama3.1:8b assessed and scored various messages from a personal dataset of Facebook messages.</p> <pre><code>YEAR|MONTH|MESSAGE|PLATFORM|SENTIMENT|SENTIMENT_SCORE\n2008|1|i really hate that {redacted} and i are still not talking. it makes me sad.|Facebook|Sadness|0.3\n2011|4|Thanks again for talking to me yesterday :)|Facebook|Appreciative|1.0\n2018|2|\u2665 Do you wanna hug because the sad satellite man?|Facebook|Silly|0.6\n\n</code></pre>"},{"location":"personal-database/","title":"Building a Personal Database","text":"<p>The first step of designing an intentional data introspection practice is to begin building a personal database of information. To do this, you will want to start by archiving and downloading your content from the social media platforms where you are active.</p> <p>Instructions for archiving content from different platforms can be found below.</p> <ul> <li>Meta platforms (Facebook, Instagram, WhatsApp)</li> <li>LinkedIn</li> <li>Twitter/X</li> </ul>"},{"location":"personal-database/#database-designs","title":"Database Designs","text":"<p>The Data Introspection Project contains several scripts for extracting content from archive files and storing them in a <code>SQLite</code> database. This is a work in progress and the database schema is likely to change, but for now, the following schema is being used:</p> <pre><code>CREATE TABLE content (\n    YEAR INTEGER,\n    MONTH INTEGER,\n    MESSAGE TEXT,\n    PLATFORM TEXT,\n    SENTIMENT TEXT,\n    SENTIMENT_SCORE REAL default 0.5);\n</code></pre> <p>If you want to be more granular in your database, include a <code>DAY INTEGER</code> or store the timestamp from the archive in a raw format. Each platform uses a different format, so you will want to convert to a consistent format if storing all content in a single database.</p> <p>Note: You may want to use different tables in your database for each platform that you important content from. Ultimately, your database design should reflect your own preferences for archival.</p>"},{"location":"personal-database/#importing-content","title":"Importing Content","text":"<p>Basic python scripts can be used to extract information from the raw archive data and save them to your database. Converting to <code>.csv</code> as an intermediate step can be helpful to reduce the amount of re-processing if you want to change your database schema. The load_to_sql.py script gives an example of how to load <code>.csv</code> files into a database using the above schema.</p>"},{"location":"personal-database/#creating-supplemental-data-tables","title":"Creating Supplemental Data Tables","text":"<p>You may want to create additional tables to join with your messages to find additional trends and perform further reflection. One example could be to create a table in <code>db.sqlite</code> that stores relationships or places that you've lived.</p> <pre><code>CREATE TABLE relationships (YEAR integer, MONTH integer, PARTNER string);\n</code></pre> <p>The above schema shows how you might represent romantic relationships over time.</p>"},{"location":"projects-we-love/","title":"Projects we Love","text":"<p>The Data Introspection Project has been inspired by some amazing data pioneers and creative technologists, some of whom are listed below in no particular order.</p> <ul> <li>Simon Willison</li> <li>Nadieh Bremer</li> <li>Shirley Wu</li> <li>Ian Johnson</li> <li>Jared Lander</li> <li>M Eilo</li> </ul>"},{"location":"related-work/","title":"Related Projects","text":""},{"location":"related-work/#archivist","title":"Archivist","text":"<p>Archivist is a desktop interface for analyzing your Facebook archives. Upload a .zip folder archive from Facebook, and Archivist will display every message you've ever sent or received. You can search and filter messages, or hook up Archivist to Ollama to ask questions about your archive and generate year by year summaries.</p> <p></p>"},{"location":"related-work/#memory-cache","title":"Memory Cache","text":"<p>Memory Cache was our first exploration into personal data archive. The Memory Cache Firefox Extension acts as a shortcut for archiving select articles and pages from your web browser, allowing you to create a local backup of any content you choose. With your Memory Cache archive, you can use a local AI agent to perform on-device retrieval augmented generation (RAG) queries against the pages you've saved from the web.</p> <p></p>"},{"location":"related-work/#stardust","title":"Stardust","text":"<p>Stardust is a multi-user chat agent that supports message synchronization between multiple users and a chat agent. Stardust can be configured to do multi-user RAG, allowing a team to easily share context and content with local embeddings to keep data private and secure.</p>"},{"location":"visualizations/","title":"Example Visualizations","text":""},{"location":"visualizations/#total-monthly-mean","title":"Total Monthly Mean","text":"<p>Graphing the mean sentiment of all messages in a monthly period with a trendline. The size of the dot corresponds to the number of messages in that period of time. Graphed using R <code>ggplot</code>. </p>"},{"location":"visualizations/#polarity-of-facebook-messages","title":"Polarity of Facebook messages","text":"<p>A trendline of positivity in Facebook messages and how it changes month over month. This graph was created using an earlier version of the sentiment scoring explorations and graphs relative change rather than absolute score. Graphed with ChatGPT. </p>"},{"location":"visualizations/#word-cloud","title":"Word Cloud","text":"<p>A word cloud generated with python using the <code>WordCloud</code> package, scaled with frequency of sentiment.  Source: gen_wordcloud.py</p>"}]}